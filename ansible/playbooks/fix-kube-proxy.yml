---
# Quick Fix Playbook: Fix kube-proxy CrashLoopBackOff
# This playbook fixes kube-proxy issues after cluster initialization
# Usage: ansible-playbook playbooks/fix-kube-proxy.yml -i inventory-devops.yml

- name: Fix kube-proxy CrashLoopBackOff
  hosts: control_plane[0]
  become: true
  gather_facts: false
  
  tasks:
    - name: Check if kubeconfig exists
      stat:
        path: /root/.kube/config
      register: kubeconfig_stat

    - name: Fail if kubeconfig not found
      fail:
        msg: "kubeconfig not found at /root/.kube/config. Please ensure cluster is initialized."
      when: not kubeconfig_stat.stat.exists

    - name: Check CNI plugin status
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-flannel -o jsonpath='{.items[*].status.phase}' 2>/dev/null || kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=calico-node -o jsonpath='{.items[*].status.phase}' 2>/dev/null || echo "CNI not found"
      register: cni_status
      changed_when: false
      failed_when: false

    - name: Install CNI plugin if not present (Flannel)
      shell: |
        kubectl --kubeconfig=/root/.kube/config apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      when: 
        - "'CNI not found' in cni_status.stdout or cni_status.stdout == ''"
        - cni_plugin | default('flannel') == "flannel"
      register: cni_install
      retries: 3
      delay: 10

    - name: Install CNI plugin if not present (Calico)
      shell: |
        curl -s https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml | \
        sed 's|# - name: CALICO_IPV4POOL_CIDR|            - name: CALICO_IPV4POOL_CIDR|' | \
        sed 's|#   value: "192.168.0.0/16"|              value: "{{ pod_network_cidr | default(''10.245.0.0/16'') }}"|' | \
        kubectl --kubeconfig=/root/.kube/config apply -f -
      when: 
        - "'CNI not found' in cni_status.stdout or cni_status.stdout == ''"
        - cni_plugin | default('flannel') == "calico"
      register: cni_install_calico
      retries: 3
      delay: 10

    - name: Wait for CNI to be ready
      shell: |
        kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s || \
        kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l k8s-app=calico-node -n kube-system --timeout=300s || true
      register: cni_wait
      retries: 10
      delay: 10
      until: cni_wait.rc == 0
      changed_when: false

    - name: Get kube-proxy pods status
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=kube-proxy -o wide
      register: kube_proxy_status
      changed_when: false

    - name: Display kube-proxy status
      debug:
        msg: "{{ kube_proxy_status.stdout_lines }}"

    - name: Delete kube-proxy pods to restart them
      shell: |
        kubectl --kubeconfig=/root/.kube/config delete pod -n kube-system -l k8s-app=kube-proxy
      register: delete_result
      changed_when: delete_result.rc == 0
      ignore_errors: yes

    - name: Wait for kube-proxy pods to restart
      pause:
        seconds: 10

    - name: Wait for kube-proxy to be ready
      shell: kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l k8s-app=kube-proxy -n kube-system --timeout=180s
      register: kube_proxy_wait
      retries: 6
      delay: 10
      until: kube_proxy_wait.rc == 0
      changed_when: false
      ignore_errors: yes

    - name: Get final kube-proxy status
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=kube-proxy
      register: final_status
      changed_when: false

    - name: Display final status
      debug:
        msg: |
          ==========================================
          kube-proxy Status:
          {{ final_status.stdout }}
          ==========================================
          If pods are still in CrashLoopBackOff, check logs:
          kubectl --kubeconfig=/root/.kube/config logs -n kube-system -l k8s-app=kube-proxy
          ==========================================

    - name: Get kube-proxy pod name
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo ""
      register: kube_proxy_pod
      changed_when: false
      failed_when: false

    - name: Check kube-proxy logs for errors
      shell: kubectl --kubeconfig=/root/.kube/config logs -n kube-system {{ kube_proxy_pod.stdout }} --tail=50 2>&1 || echo "Could not get logs"
      register: kube_proxy_logs
      changed_when: false
      failed_when: false
      when: kube_proxy_pod.stdout != ""

    - name: Display kube-proxy error logs
      debug:
        msg: "{{ kube_proxy_logs.stdout_lines }}"
      when: kube_proxy_logs is defined

    - name: Check kube-proxy describe for more details
      shell: kubectl --kubeconfig=/root/.kube/config describe pod -n kube-system {{ kube_proxy_pod.stdout }} 2>&1 | grep -A 20 "Events:" || echo "Could not describe pod"
      register: kube_proxy_describe
      changed_when: false
      failed_when: false
      when: kube_proxy_pod.stdout != ""

    - name: Display kube-proxy describe events
      debug:
        msg: "{{ kube_proxy_describe.stdout_lines }}"
      when: kube_proxy_describe is defined

    - name: Check if issue is CNI-related
      debug:
        msg: |
          ==========================================
          TROUBLESHOOTING TIPS:
          ==========================================
          1. If logs show "connection refused" or "no route to host":
             - CNI plugin may not be ready
             - Check: kubectl get pods -n kube-flannel
             - Wait for CNI pods to be Running
          
          2. If logs show "image pull" errors:
             - Check network connectivity
             - Verify image registry is accessible
          
          3. If logs show "permission denied" or "access denied":
             - Check RBAC configuration
             - Verify service account permissions
          
          4. Common fix sequence:
             a. Ensure CNI is installed and ready
             b. Delete kube-proxy pods to restart
             c. Wait for pods to come back up
          ==========================================
      when: "'CrashLoopBackOff' in final_status.stdout or 'Error' in final_status.stdout"

