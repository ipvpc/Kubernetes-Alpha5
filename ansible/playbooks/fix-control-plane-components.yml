---
# Fix Control Plane Components (API Server, Scheduler, Controller Manager)
# This playbook fixes issues with control plane static pods
# Usage: ansible-playbook playbooks/fix-control-plane-components.yml -i inventory-devops.yml

- name: Fix Control Plane Components
  hosts: control_plane[0]
  become: true
  gather_facts: false
  
  tasks:
    - name: Check if admin.conf exists
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf_exists

    - name: Get master node IP
      set_fact:
        master_ip: "{{ hostvars[groups['control_plane'][0]]['ansible_host'] }}"

    - name: Check all control plane pods status
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l tier=control-plane -o wide 2>&1 || echo "CANNOT_QUERY"
      register: control_plane_pods
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display control plane pods status
      debug:
        msg: "{{ control_plane_pods.stdout_lines }}"
      when: control_plane_pods is defined

    - name: Check API server logs
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf logs -n kube-system -l component=kube-apiserver --tail=30 2>&1 || journalctl -u kubelet --no-pager | grep -i "apiserver" | tail -20 || echo "CANNOT_GET_LOGS"
      register: api_logs
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display API server logs
      debug:
        msg: "{{ api_logs.stdout_lines }}"
      when: api_logs is defined

    - name: Check kube-scheduler logs
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf logs -n kube-system -l component=kube-scheduler --tail=30 2>&1 || journalctl -u kubelet --no-pager | grep -i "scheduler" | tail -20 || echo "CANNOT_GET_LOGS"
      register: scheduler_logs
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display kube-scheduler logs
      debug:
        msg: "{{ scheduler_logs.stdout_lines }}"
      when: scheduler_logs is defined

    - name: Check kube-controller-manager logs
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf logs -n kube-system -l component=kube-controller-manager --tail=30 2>&1 || journalctl -u kubelet --no-pager | grep -i "controller" | tail -20 || echo "CANNOT_GET_LOGS"
      register: controller_logs
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display kube-controller-manager logs
      debug:
        msg: "{{ controller_logs.stdout_lines }}"
      when: controller_logs is defined

    - name: Check API server static pod manifest
      shell: |
        echo "=== API Server Manifest ==="
        cat /etc/kubernetes/manifests/kube-apiserver.yaml 2>/dev/null | grep -E '--bind-address|--advertise-address|--secure-port' || echo "Manifest not found"
      register: api_manifest_check
      changed_when: false
      failed_when: false

    - name: Display API server manifest
      debug:
        msg: "{{ api_manifest_check.stdout_lines }}"

    - name: Check kube-scheduler static pod manifest
      shell: |
        echo "=== Scheduler Manifest ==="
        cat /etc/kubernetes/manifests/kube-scheduler.yaml 2>/dev/null | head -30 || echo "Manifest not found"
      register: scheduler_manifest_check
      changed_when: false
      failed_when: false

    - name: Display scheduler manifest
      debug:
        msg: "{{ scheduler_manifest_check.stdout_lines }}"

    - name: Check if API server is accessible
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw=/healthz 2>&1
      register: api_health
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display API server health
      debug:
        msg: "API server health: {{ api_health.stdout }}"
      when: api_health is defined

    - name: Check if API server is listening
      shell: |
        ss -tlnp | grep ':6443' || netstat -tlnp | grep ':6443' || echo "NOT_LISTENING"
      register: api_listening
      changed_when: false
      failed_when: false

    - name: Display API server listening status
      debug:
        msg: "API server listening: {{ api_listening.stdout }}"

    - name: Fix API server bind address if needed
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--bind-address=127\.0\.0\.1'
        replace: '    - --bind-address=0.0.0.0'
      register: fix_api_bind
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Fix API server advertise address if needed
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--advertise-address=127\.0\.0\.1'
        replace: '    - --advertise-address={{ master_ip }}'
      register: fix_api_advertise
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Check kube-scheduler kubeconfig path
      shell: |
        grep -E 'kubeconfig|--kubeconfig' /etc/kubernetes/manifests/kube-scheduler.yaml || echo "kubeconfig not found in manifest"
      register: scheduler_kubeconfig
      changed_when: false
      failed_when: false

    - name: Display scheduler kubeconfig
      debug:
        msg: "Scheduler kubeconfig: {{ scheduler_kubeconfig.stdout }}"

    - name: Check if kube-scheduler kubeconfig exists
      stat:
        path: /etc/kubernetes/scheduler.conf
      register: scheduler_conf_exists

    - name: Check kube-controller-manager kubeconfig
      stat:
        path: /etc/kubernetes/controller-manager.conf
      register: controller_conf_exists

    - name: Verify all kubeconfigs exist
      debug:
        msg: |
          API Server admin.conf: {{ admin_conf_exists.stat.exists }}
          Scheduler config: {{ scheduler_conf_exists.stat.exists }}
          Controller Manager config: {{ controller_conf_exists.stat.exists }}

    - name: Restart kubelet to reload static pod manifests
      systemd:
        name: kubelet
        state: restarted
      when: fix_api_bind.changed | default(false) or fix_api_advertise.changed | default(false)
      register: restart_kubelet

    - name: Wait for kubelet to restart
      pause:
        seconds: 10
      when: restart_kubelet.changed | default(false)

    - name: Wait for API server to be ready
      shell: |
        for i in {1..60}; do
          if kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw=/healthz 2>/dev/null | grep -q "ok"; then
            exit 0
          fi
          sleep 2
        done
        exit 1
      register: wait_api
      retries: 1
      delay: 0
      until: wait_api.rc == 0
      changed_when: false
      ignore_errors: yes
      when: 
        - admin_conf_exists.stat.exists
        - restart_kubelet.changed | default(false)

    - name: Wait for control plane pods to be ready
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod -l tier=control-plane -n kube-system --timeout=300s
      register: wait_control_plane
      retries: 3
      delay: 10
      until: wait_control_plane.rc == 0
      changed_when: false
      ignore_errors: yes
      when: admin_conf_exists.stat.exists

    - name: Get final control plane pods status
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l tier=control-plane -o wide
      register: final_status
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display final status
      debug:
        msg: |
          ==========================================
          Control Plane Components Status:
          {{ final_status.stdout }}
          ==========================================
          
          If pods are still failing:
          1. Check logs: kubectl --kubeconfig=/etc/kubernetes/admin.conf logs -n kube-system <pod-name>
          2. Check kubelet: journalctl -u kubelet -n 100
          3. Verify API server: kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info
          ==========================================
      when: final_status is defined

    - name: Check etcd status
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l component=etcd -o wide 2>&1 || echo "etcd check failed"
      register: etcd_status
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display etcd status
      debug:
        msg: "{{ etcd_status.stdout_lines }}"
      when: etcd_status is defined

    - name: Display comprehensive diagnosis
      debug:
        msg: |
          ==========================================
          Control Plane Diagnosis Summary
          ==========================================
          
          Issues Found:
          {% if api_health.rc != 0 %}
          - API server is not responding to health checks
          {% endif %}
          {% if api_listening.stdout == "NOT_LISTENING" %}
          - API server is not listening on port 6443
          {% endif %}
          {% if not scheduler_conf_exists.stat.exists %}
          - Scheduler kubeconfig missing
          {% endif %}
          {% if not controller_conf_exists.stat.exists %}
          - Controller manager kubeconfig missing
          {% endif %}
          
          Actions Taken:
          {% if fix_api_bind.changed %}
          - Fixed API server bind address
          {% endif %}
          {% if fix_api_advertise.changed %}
          - Fixed API server advertise address
          {% endif %}
          {% if restart_kubelet.changed %}
          - Restarted kubelet
          {% endif %}
          
          Next Steps:
          1. Wait 2-3 minutes for all pods to restart
          2. Check status: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -A
          3. If issues persist, check individual component logs
          ==========================================

