---
# Ansible Playbook: Install Kubernetes using kubeadm
# This playbook installs Kubernetes on remote hosts using kubeadm

- name: Install Kubernetes with kubeadm
  hosts: all
  become: true
  gather_facts: true
  
  vars:
    kubeadm_init_token: "{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits') }}"
    
  tasks:
    - name: Check if running on supported OS
      assert:
        that:
          - ansible_os_family == "Debian" or ansible_os_family == "RedHat"
        fail_msg: "Unsupported OS family: {{ ansible_os_family }}"
        success_msg: "OS family {{ ansible_os_family }} is supported"

    - name: Disable swap
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
      when: disable_swap | bool
      changed_when: false

    - name: Load required kernel modules
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter
      when: load_kernel_modules | bool

    - name: Configure sysctl parameters
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        sysctl_file: /etc/sysctl.d/99-kubernetes.conf
        reload: true
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }
      when: enable_ip_forwarding | bool

    - name: Install required packages (Debian/Ubuntu)
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
        state: present
        update_cache: true
      when: ansible_os_family == "Debian"

    - name: Install required packages (RedHat/CentOS)
      yum:
        name:
          - yum-utils
          - device-mapper-persistent-data
          - lvm2
        state: present
      when: ansible_os_family == "RedHat"

    - name: Add Kubernetes GPG key (Debian/Ubuntu)
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key
        state: present
      when: ansible_os_family == "Debian"

    - name: Add Kubernetes repository (Debian/Ubuntu)
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == "Debian"

    - name: Add Kubernetes repository (RedHat/CentOS)
      yum_repository:
        name: kubernetes
        description: Kubernetes
        baseurl: https://pkgs.k8s.io/core:/stable:/v1.28/rpm/
        gpgcheck: yes
        gpgkey: https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key
        enabled: yes
      when: ansible_os_family == "RedHat"

    - name: Install container runtime (containerd)
      block:
        - name: Install containerd (Debian/Ubuntu)
          apt:
            name:
              - containerd
              - cri-tools
            state: present
          when: ansible_os_family == "Debian"

        - name: Install containerd (RedHat/CentOS)
          yum:
            name:
              - containerd.io
              - cri-tools
            state: present
          when: ansible_os_family == "RedHat"

        - name: Create containerd config directory
          file:
            path: /etc/containerd
            state: directory
            mode: '0755'

        - name: Generate default containerd config
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml
          changed_when: false

        - name: Update pause image to version 3.9
          replace:
            path: /etc/containerd/config.toml
            regexp: 'sandbox_image = ".*"'
            replace: 'sandbox_image = "registry.k8s.io/pause:3.9"'

        - name: Ensure SystemdCgroup is enabled
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'

        - name: Ensure SystemdCgroup is set if not present
          lineinfile:
            path: /etc/containerd/config.toml
            regexp: '^\s*SystemdCgroup\s*='
            line: '            SystemdCgroup = true'
            insertafter: '\[plugins\."io\.containerd\.grpc\.v1\.cri"\.containerd\.runtimes\.runc\.options\]'
            backup: yes

        - name: Restart containerd to apply configuration
          systemd:
            name: containerd
            state: restarted
            enabled: true

        - name: Wait for containerd to be ready
          wait_for:
            path: /var/run/containerd/containerd.sock
            timeout: 30
      when: container_runtime == "containerd"

    - name: Install kubeadm, kubelet, kubectl
      apt:
        name:
          - kubelet={{ kubernetes_version }}-1.1
          - kubectl={{ kubernetes_version }}-1.1
          - kubeadm={{ kubernetes_version }}-1.1
        state: present
        allow_downgrades: yes
        allow_change_held_packages: yes
      when: ansible_os_family == "Debian"

    - name: Install kubeadm, kubelet, kubectl (RedHat)
      yum:
        name:
          - kubelet-{{ kubernetes_version }}-1.1
          - kubectl-{{ kubernetes_version }}-1.1
          - kubeadm-{{ kubernetes_version }}-1.1
        state: present
      when: ansible_os_family == "RedHat"

    - name: Hold kubelet, kubeadm, kubectl (Debian/Ubuntu)
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == "Debian"

    - name: Enable kubelet service
      systemd:
        name: kubelet
        enabled: true
        daemon_reload: true

    - name: Configure firewall (UFW)
      ufw:
        rule: allow
        port: "{{ item.port }}"
        proto: "{{ item.protocol }}"
      loop: "{{ firewall_ports }}"
      when: 
        - configure_firewall | bool
        - ansible_os_family == "Debian"

    - name: Configure firewall (firewalld)
      firewalld:
        port: "{{ item.port }}/{{ item.protocol }}"
        permanent: true
        state: enabled
        immediate: true
      loop: "{{ firewall_ports }}"
      when:
        - configure_firewall | bool
        - ansible_os_family == "RedHat"

  handlers:
    - name: restart containerd
      systemd:
        name: containerd
        state: restarted
      listen: restart_containerd

- name: Initialize Kubernetes cluster (first master only)
  hosts: control_plane[0]
  become: true
  gather_facts: false
  
  tasks:
    - name: Get master node IP
      set_fact:
        master_ip: "{{ hostvars[groups['control_plane'][0]]['ansible_host'] }}"

    - name: Set CRI socket path
      set_fact:
        cri_socket: "{% if container_runtime == 'containerd' %}unix:///var/run/containerd/containerd.sock{% elif container_runtime == 'crio' %}unix:///var/run/crio/crio.sock{% else %}unix:///var/run/containerd/containerd.sock{% endif %}"

    - name: Check if Kubernetes is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig_exists

    - name: Reset Kubernetes cluster if already initialized
      shell: kubeadm reset --force --cri-socket={{ cri_socket }}
      when: kubeconfig_exists.stat.exists
      ignore_errors: yes

    - name: Verify containerd is running
      systemd:
        name: containerd
        state: started
      register: containerd_status

    - name: Wait for containerd socket
      wait_for:
        path: /var/run/containerd/containerd.sock
        timeout: 30

    - name: Test containerd CRI interface
      shell: |
        crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock version
      register: crictl_test
      ignore_errors: yes
      changed_when: false

    - name: Display containerd CRI status
      debug:
        msg: "Containerd CRI interface: {{ 'Working' if crictl_test.rc == 0 else 'Not responding - this may cause issues' }}"

    - name: Pre-pull Kubernetes images before init
      shell: |
        kubeadm config images pull --kubernetes-version={{ kubernetes_version }} --cri-socket={{ cri_socket }}
      ignore_errors: yes
      changed_when: false

    - name: Check for partial cluster initialization
      stat:
        path: /etc/kubernetes/manifests
      register: manifests_exist

    - name: Clean up partial initialization if needed
      shell: |
        if [ -d /etc/kubernetes/manifests ] && [ ! -f /etc/kubernetes/admin.conf ]; then
          kubeadm reset --force --cri-socket={{ cri_socket }} || true
          rm -rf /etc/kubernetes/manifests/* || true
        fi
      when: manifests_exist.stat.exists
      ignore_errors: yes
      changed_when: false

    - name: Initialize Kubernetes cluster
      shell: >
        timeout 600 kubeadm init
        --pod-network-cidr={{ pod_network_cidr }}
        --service-cidr={{ service_cidr }}
        --control-plane-endpoint={{ master_ip }}:6443
        --upload-certs
        --cri-socket={{ cri_socket }}
        --image-repository=registry.k8s.io
        --apiserver-advertise-address={{ master_ip }}
        --apiserver-bind-port=6443
      register: kubeadm_init
      changed_when: true
      failed_when: false
      async: 600
      poll: 10

    - name: Check if admin.conf was created (cluster may be partially initialized)
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf_exists

    - name: Fix API server bind address immediately after init
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--bind-address=127\.0\.0\.1'
        replace: '    - --bind-address=0.0.0.0'
      register: fix_api_bind_immediate
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Fix API server advertise address immediately after init
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--advertise-address=127\.0\.0\.1'
        replace: '    - --advertise-address={{ master_ip }}'
      register: fix_api_advertise_immediate
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Restart kubelet if API server config was fixed
      systemd:
        name: kubelet
        state: restarted
      when: 
        - admin_conf_exists.stat.exists
        - fix_api_bind_immediate.changed | default(false) or fix_api_advertise_immediate.changed | default(false)

    - name: Wait for API server to restart after bind address fix
      pause:
        seconds: 30
      when: 
        - admin_conf_exists.stat.exists
        - fix_api_bind_immediate.changed | default(false) or fix_api_advertise_immediate.changed | default(false)

    - name: Check if cluster is usable despite kubeadm init error
      shell: |
        if [ -f /etc/kubernetes/admin.conf ]; then
          kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes 2>/dev/null && echo "CLUSTER_USABLE" || echo "CLUSTER_NOT_READY"
        else
          echo "NO_ADMIN_CONF"
        fi
      register: cluster_usable_check
      changed_when: false
      failed_when: false
      when: kubeadm_init.rc != 0

    - name: Handle kubeadm init failure
      block:
        - name: Fail if cluster is not usable
          fail:
            msg: |
              kubeadm init failed with return code {{ kubeadm_init.rc }}
              Error output: {{ kubeadm_init.stderr | default('No error output') }}
              admin.conf was not created. The cluster did not initialize.
              Consider running: kubeadm reset --force --cri-socket={{ cri_socket }}
          when: 
            - kubeadm_init.rc != 0
            - not admin_conf_exists.stat.exists

        - name: Warn about partial initialization but continue
          debug:
            msg: |
              WARNING: kubeadm init failed (return code {{ kubeadm_init.rc }}), but admin.conf exists.
              The cluster may be partially initialized (CoreDNS may be missing).
              Continuing with installation - we will install CoreDNS manually if needed.
              Error: {{ kubeadm_init.stderr | default('No error output') }}
          when: 
            - kubeadm_init.rc != 0
            - admin_conf_exists.stat.exists
            - cluster_usable_check.stdout != "CLUSTER_USABLE"
      when: kubeadm_init.rc != 0

    - name: Debug kubeadm init output
      debug:
        msg: "{{ kubeadm_init.stdout_lines }}"
      when: kubeadm_init.stdout_lines is defined

    - name: Extract all kubeadm join commands from stdout
      set_fact:
        all_join_commands_stdout: "{{ kubeadm_init.stdout_lines | default([]) | select('match', 'kubeadm join') | list }}"

    - name: Extract all kubeadm join commands from stderr
      set_fact:
        all_join_commands_stderr: "{{ kubeadm_init.stderr_lines | default([]) | select('match', 'kubeadm join') | list }}"

    - name: Combine join commands from stdout and stderr
      set_fact:
        all_join_commands: "{{ (all_join_commands_stdout | default([])) + (all_join_commands_stderr | default([])) }}"

    - name: Debug join commands found
      debug:
        msg: "Found join commands: {{ all_join_commands }}"
      when: all_join_commands is defined and all_join_commands | length > 0

    - name: Generate join token if join commands not found
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf create token --ttl=24h --print-join-command 2>/dev/null || \
        kubeadm token create --print-join-command 2>/dev/null || echo "TOKEN_CREATE_FAILED"
      register: generated_join_command
      changed_when: generated_join_command.rc == 0
      failed_when: false
      when: 
        - admin_conf_exists.stat.exists
        - all_join_commands | default([]) | length == 0

    - name: Extract join command from generated token
      set_fact:
        generated_join_cmd: "{{ generated_join_command.stdout | default('') | trim }}"
      when: 
        - generated_join_command is defined
        - generated_join_command.rc == 0
        - "'TOKEN_CREATE_FAILED' not in generated_join_command.stdout"

    - name: Get CA cert hash for join command
      shell: |
        openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' || echo "HASH_FAILED"
      register: ca_cert_hash
      changed_when: false
      failed_when: false
      when: 
        - admin_conf_exists.stat.exists
        - all_join_commands | default([]) | length == 0

    - name: Get bootstrap token
      shell: |
        kubeadm token list -o jsonpath='{.items[0].token}' 2>/dev/null || echo "TOKEN_NOT_FOUND"
      register: bootstrap_token
      changed_when: false
      failed_when: false
      when: 
        - admin_conf_exists.stat.exists
        - all_join_commands | default([]) | length == 0

    - name: Build join command manually if needed
      set_fact:
        manual_join_command: "kubeadm join {{ master_ip }}:6443 --token {{ bootstrap_token.stdout }} --discovery-token-ca-cert-hash sha256:{{ ca_cert_hash.stdout }} --cri-socket={{ cri_socket }}"
      when: 
        - admin_conf_exists.stat.exists
        - all_join_commands | default([]) | length == 0
        - bootstrap_token.stdout != "TOKEN_NOT_FOUND"
        - ca_cert_hash.stdout != "HASH_FAILED"

    - name: Save kubeadm join command for control plane
      set_fact:
        kubeadm_join_command: "{{ (all_join_commands[0] | default(generated_join_cmd | default(manual_join_command | default('')))) | string | trim }}"
      when: admin_conf_exists.stat.exists

    - name: Save kubeadm join command for workers
      set_fact:
        kubeadm_join_command_worker: "{{ (all_join_commands[-1] | default(all_join_commands[0] | default(generated_join_cmd | default(manual_join_command | default(''))))) | string | trim }}"
      when: admin_conf_exists.stat.exists

    - name: Display final join commands
      debug:
        msg: |
          Control plane join command: {{ kubeadm_join_command | default('NOT_SET') }}
          Worker join command: {{ kubeadm_join_command_worker | default('NOT_SET') }}
      when: admin_conf_exists.stat.exists

    - name: Create .kube directory
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

    - name: Copy kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        mode: '0644'

    - name: Verify kubeconfig file exists
      stat:
        path: /root/.kube/config
      register: kubeconfig_stat

    - name: Check kubeconfig server endpoint
      shell: grep -E '^\s*server:' /root/.kube/config | head -1
      register: kubeconfig_server
      changed_when: false
      failed_when: false
      when: kubeconfig_stat.stat.exists

    - name: Display kubeconfig server endpoint
      debug:
        msg: "Current kubeconfig server: {{ kubeconfig_server.stdout | default('Not found') }}"

    - name: Update kubeconfig server endpoint if needed
      replace:
        path: /root/.kube/config
        regexp: '^\s*server:\s*https://127\.0\.0\.1:6443'
        replace: '    server: https://{{ master_ip }}:6443'
      when: 
        - kubeconfig_stat.stat.exists
        - "'127.0.0.1' in kubeconfig_server.stdout | default('')"

    - name: Also update localhost references to master IP
      replace:
        path: /root/.kube/config
        regexp: '^\s*server:\s*https://localhost:6443'
        replace: '    server: https://{{ master_ip }}:6443'
      when: kubeconfig_stat.stat.exists

    - name: Check if API server is listening on port 6443
      shell: |
        netstat -tlnp 2>/dev/null | grep ':6443' || ss -tlnp 2>/dev/null | grep ':6443' || echo "Port check failed"
      register: port_check
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display port check results
      debug:
        msg: "Port 6443 status: {{ port_check.stdout }}"
      when: 
        - kubeconfig_stat.stat.exists
        - port_check is defined

    - name: Wait for API server static pod manifest
      wait_for:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        timeout: 60
      when: admin_conf_exists.stat.exists

    - name: Check API server manifest bind address
      shell: grep -E '^\s*-\s*--bind-address' /etc/kubernetes/manifests/kube-apiserver.yaml || echo "bind-address not found"
      register: api_bind_address
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display API server bind address
      debug:
        msg: "API server bind address: {{ api_bind_address.stdout }}"

    - name: Fix API server bind address if binding to localhost
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--bind-address=127\.0\.0\.1'
        replace: '    - --bind-address=0.0.0.0'
      when: 
        - admin_conf_exists.stat.exists
        - "'127.0.0.1' in api_bind_address.stdout"
      register: bind_address_fix

    - name: Ensure advertise address is set correctly
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '^\s*-\s*--advertise-address=127\.0\.0\.1'
        replace: '    - --advertise-address={{ master_ip }}'
      when: 
        - admin_conf_exists.stat.exists
        - "'127.0.0.1' in api_bind_address.stdout"
      register: advertise_address_fix

    - name: Add bind address if missing
      lineinfile:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        line: '    - --bind-address=0.0.0.0'
        insertafter: '^\s*-\s*--authorization-mode'
        backup: yes
      when: 
        - admin_conf_exists.stat.exists
        - api_bind_address.stdout == "bind-address not found"
      register: add_bind_address

    - name: Add advertise address if missing
      lineinfile:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        line: '    - --advertise-address={{ master_ip }}'
        insertafter: '^\s*-\s*--bind-address'
        backup: yes
      when: 
        - admin_conf_exists.stat.exists
        - api_bind_address.stdout == "bind-address not found"
      register: add_advertise_address

    - name: Restart kubelet if API server config was changed
      systemd:
        name: kubelet
        state: restarted
      when: 
        - admin_conf_exists.stat.exists
        - bind_address_fix.changed | default(false) or advertise_address_fix.changed | default(false) or add_bind_address.changed | default(false) or add_advertise_address.changed | default(false)

    - name: Wait for API server static pod to be created by kubelet
      shell: |
        for i in {1..60}; do
          if [ -f /etc/kubernetes/manifests/kube-apiserver.yaml ]; then
            # Check if kubelet has created the pod directory
            if [ -d /var/lib/kubelet/pods ] && [ -n "$(find /var/lib/kubelet/pods -name '*kube-apiserver*' -type d 2>/dev/null | head -1)" ]; then
              exit 0
            fi
          fi
          sleep 2
        done
        exit 1
      register: api_server_pod_wait
      retries: 1
      delay: 0
      until: api_server_pod_wait.rc == 0
      changed_when: false
      ignore_errors: yes
      when: admin_conf_exists.stat.exists

    - name: Wait for API server to respond (using admin.conf)
      shell: |
        for i in {1..60}; do
          if kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw=/healthz 2>/dev/null | grep -q "ok"; then
            exit 0
          fi
          sleep 5
        done
        exit 1
      register: api_server_health_wait
      retries: 1
      delay: 0
      until: api_server_health_wait.rc == 0
      changed_when: false
      ignore_errors: yes
      when: admin_conf_exists.stat.exists

    - name: Check API server pod status (using admin.conf)
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l component=kube-apiserver -o wide 2>/dev/null || echo "API server check failed"
      register: api_server_status
      changed_when: false
      failed_when: false
      when: admin_conf_exists.stat.exists

    - name: Display API server status
      debug:
        msg: "{{ api_server_status.stdout_lines }}"
      when: 
        - admin_conf_exists.stat.exists
        - api_server_status is defined

    - name: Check API server pod status (using copied config as fallback)
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l component=kube-apiserver -o wide 2>/dev/null || echo "API server check failed"
      register: api_server_status2
      changed_when: false
      failed_when: false
      when: 
        - kubeconfig_stat.stat.exists
        - not admin_conf_exists.stat.exists

    - name: Wait for API server to be ready (using admin.conf directly)
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
      register: wait_result
      until: wait_result.rc == 0
      retries: 60
      delay: 5
      changed_when: false
      when: admin_conf_exists.stat.exists

    - name: Install CoreDNS manually if kubeadm init failed
      shell: |
        # Check if CoreDNS already exists
        if ! kubectl --kubeconfig=/etc/kubernetes/admin.conf get deployment coredns -n kube-system 2>/dev/null; then
          # Get CoreDNS manifest from kubeadm
          kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f https://raw.githubusercontent.com/coredns/coredns/master/kubernetes/coredns.yaml.sed || \
          kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f https://github.com/coredns/coredns/releases/download/v1.10.1/coredns.yaml || \
          echo "CoreDNS installation failed - will retry"
        else
          echo "CoreDNS already exists"
        fi
      register: coredns_install
      retries: 3
      delay: 10
      until: coredns_install.rc == 0
      changed_when: coredns_install.rc == 0
      ignore_errors: yes
      when: 
        - admin_conf_exists.stat.exists
        - kubeadm_init.rc != 0 | default(false)
        - wait_result.rc == 0 | default(false)

    - name: Wait for CoreDNS to be ready
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod -l k8s-app=kube-dns -n kube-system --timeout=180s
      register: coredns_wait
      retries: 6
      delay: 10
      until: coredns_wait.rc == 0
      changed_when: false
      ignore_errors: yes
      when: 
        - admin_conf_exists.stat.exists
        - coredns_install is defined
        - coredns_install.rc == 0 | default(false)

    - name: Wait for API server to be ready (using copied config)
      shell: kubectl --kubeconfig=/root/.kube/config get nodes
      register: wait_result2
      until: wait_result2.rc == 0
      retries: 10
      delay: 3
      changed_when: false
      when: 
        - kubeconfig_stat.stat.exists
        - wait_result.rc == 0 | default(false)

    - name: Verify API server connectivity
      shell: kubectl --kubeconfig=/root/.kube/config cluster-info
      register: cluster_info
      changed_when: false
      failed_when: false
      when: kubeconfig_stat.stat.exists

    - name: Display cluster info
      debug:
        msg: "{{ cluster_info.stdout_lines }}"
      when: 
        - kubeconfig_stat.stat.exists
        - cluster_info is defined

    - name: Install CNI plugin (Flannel)
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml || \
        kubectl --kubeconfig=/root/.kube/config apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      register: cni_install
      retries: 3
      delay: 10
      until: cni_install.rc == 0
      when: 
        - cni_plugin == "flannel"
        - admin_conf_exists.stat.exists or kubeconfig_stat.stat.exists

    - name: Install CNI plugin (Calico)
      shell: |
        KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml || \
        kubectl --kubeconfig=/root/.kube/config apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml
      register: cni_install_calico
      retries: 3
      delay: 10
      until: cni_install_calico.rc == 0
      when:
        - cni_plugin == "calico"
        - admin_conf_exists.stat.exists or kubeconfig_stat.stat.exists

    - name: Wait for CNI pods to be ready
      shell: |
        if [ "{{ cni_plugin | default('flannel') }}" == "calico" ]; then
          KUBECONFIG=/etc/kubernetes/admin.conf kubectl wait --for=condition=ready pod -l k8s-app=calico-node -n kube-system --timeout=300s || \
          kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l k8s-app=calico-node -n kube-system --timeout=300s
        else
          KUBECONFIG=/etc/kubernetes/admin.conf kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s || \
          kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s
        fi
      register: cni_wait
      retries: 10
      delay: 10
      until: cni_wait.rc == 0
      changed_when: false
      when: admin_conf_exists.stat.exists or kubeconfig_stat.stat.exists

    - name: Check kube-proxy status
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[*].status.phase}'
      register: kube_proxy_status
      changed_when: false
      failed_when: false
      when: kubeconfig_stat.stat.exists

    - name: Restart kube-proxy if in CrashLoopBackOff
      shell: |
        kubectl --kubeconfig=/root/.kube/config delete pod -n kube-system -l k8s-app=kube-proxy
        sleep 5
      when: 
        - kubeconfig_stat.stat.exists
        - "'CrashLoopBackOff' in kube_proxy_status.stdout or 'Error' in kube_proxy_status.stdout"

    - name: Wait for kube-proxy to be ready
      shell: kubectl --kubeconfig=/root/.kube/config wait --for=condition=ready pod -l k8s-app=kube-proxy -n kube-system --timeout=180s
      register: kube_proxy_wait
      retries: 6
      delay: 10
      until: kube_proxy_wait.rc == 0
      changed_when: false
      ignore_errors: yes
      when: kubeconfig_stat.stat.exists

    - name: Display kube-proxy status
      shell: kubectl --kubeconfig=/root/.kube/config get pods -n kube-system -l k8s-app=kube-proxy
      register: kube_proxy_pods
      changed_when: false
      when: kubeconfig_stat.stat.exists

    - name: Show kube-proxy pod status
      debug:
        msg: "{{ kube_proxy_pods.stdout_lines }}"
      when: 
        - kubeconfig_stat.stat.exists
        - kube_proxy_pods is defined

- name: Join additional control plane nodes
  hosts: control_plane[1:]
  become: true
  gather_facts: false
  
  tasks:
    - name: Join control plane node
      shell: "{{ hostvars[groups['control_plane'][0]]['kubeadm_join_command'] }}"
      when: kubeadm_join_command is defined

- name: Join worker nodes
  hosts: workers
  become: true
  gather_facts: false
  
  tasks:
    - name: Set CRI socket path
      set_fact:
        cri_socket: "{% if container_runtime == 'containerd' %}unix:///var/run/containerd/containerd.sock{% elif container_runtime == 'crio' %}unix:///var/run/crio/crio.sock{% else %}unix:///var/run/containerd/containerd.sock{% endif %}"

    - name: Reset worker node if previously joined
      shell: kubeadm reset --force --cri-socket={{ cri_socket }}
      ignore_errors: yes
      changed_when: false

    - name: Get join command from master
      set_fact:
        join_command_raw: "{{ hostvars[groups['control_plane'][0]]['kubeadm_join_command_worker'] | default('') }}"

    - name: Clean join command (ensure it's a string and remove trailing backslashes)
      set_fact:
        join_command: "{{ (join_command_raw | default('')) | string | replace('[', '') | replace(']', '') | replace(\"'\", '') | replace('\"', '') | regex_replace('\\\\\\s*$', '') | regex_replace('\\s+', ' ') | regex_replace('^,|,$', '') | trim }}"

    - name: Build complete join command with CRI socket and CA verification
      set_fact:
        join_command_with_cri: >-
          {% if '--cri-socket' not in join_command %}
          {{ join_command }} --cri-socket={{ cri_socket }}
          {% else %}
          {{ join_command }}
          {% endif %}
          {% if '--discovery-token-ca-cert-hash' not in join_command and '--discovery-token-unsafe-skip-ca-verification' not in join_command %}
          --discovery-token-unsafe-skip-ca-verification
          {% endif %}
      when: join_command != ""

    - name: Debug join command
      debug:
        msg: "Join command for {{ inventory_hostname }}: {{ join_command_with_cri | default(join_command) }}"
      when: join_command != ""

    - name: Fail if join command is empty
      fail:
        msg: "Join command is empty. Check if kubeadm init completed successfully on master node."
      when: join_command == ""

    - name: Join worker node
      shell: "{{ join_command_with_cri | default(join_command) }}"
      when: join_command != ""
      register: join_result

    - name: Display join result
      debug:
        msg: "{{ join_result.stdout_lines }}"
      when: 
        - join_result is defined
        - join_result.stdout_lines is defined
